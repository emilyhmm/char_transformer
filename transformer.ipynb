{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e2def0f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "e2def0f4",
        "outputId": "9bf64ab6-ca97-4ab2-f32c-3a1a8b813db3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2460807974.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Enable autoreload of local Python modules (e.g., models)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-57>\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/extensions/autoreload.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msource_from_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m#------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Enable autoreload of local Python modules (e.g., models)\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d5001b05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "d5001b05",
        "outputId": "200e7b76-a86d-4ee5-f9d8-7b068cab27e6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2761168333.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# local imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import time\n",
        "\n",
        "# local imports\n",
        "import models.models as models\n",
        "import util.generation as generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906db4f2",
      "metadata": {
        "id": "906db4f2"
      },
      "outputs": [],
      "source": [
        "# initialize the jax random key\n",
        "key = jax.random.key(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762a1c8f",
      "metadata": {
        "id": "762a1c8f"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86825275",
      "metadata": {
        "id": "86825275"
      },
      "outputs": [],
      "source": [
        "# load the ./data/text8_train.txt and ./data/text8_test.txt files\n",
        "with open(\"./data/text8_train.txt\", \"r\") as f:\n",
        "    train_text = f.read()\n",
        "with open(\"./data/text8_test.txt\", \"r\") as f:\n",
        "    test_text = f.read()\n",
        "\n",
        "# print the length of the training text and test text\n",
        "print(f\"Length of training text: {len(train_text):_} characters\")\n",
        "print(f\"Length of test text: {len(test_text):_} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfdca63",
      "metadata": {
        "id": "7bfdca63"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34b1eafe",
      "metadata": {
        "id": "34b1eafe"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary (lowercase + space + a few punctuations)\n",
        "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
        "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
        "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
        "\n",
        "def encode(s):\n",
        "    \"\"\"Encode string to array of integers\"\"\"\n",
        "    ids = [char_to_int[c] for c in s]\n",
        "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b5af70",
      "metadata": {
        "id": "42b5af70"
      },
      "outputs": [],
      "source": [
        "# encode the text\n",
        "train_text_int = encode(train_text)\n",
        "test_text_int = encode(test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6458536d",
      "metadata": {
        "id": "6458536d"
      },
      "outputs": [],
      "source": [
        "# sanity check: display a few random characters from the training text\n",
        "T = 128\n",
        "for _ in range(5):\n",
        "    # choose random position in text\n",
        "    N = np.random.randint(low=0, high=len(train_text)-T)\n",
        "    print(train_text[N:N+T])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7724c34b",
      "metadata": {
        "id": "7724c34b"
      },
      "source": [
        "# Create a basic Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5955c3f",
      "metadata": {
        "id": "f5955c3f"
      },
      "outputs": [],
      "source": [
        "def create_train_state(rng, vocab_size=27, d_model=64, n_layers=6, n_heads=8, max_len=128):\n",
        "    # create a basic Transformer model\n",
        "    model = models.DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, max_len)\n",
        "    # create a dummy input for initialization\n",
        "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
        "    # pass the dummy input to the model to initialize the parameters\n",
        "    params = model.init({\"params\": rng}, dummy)[\"params\"]\n",
        "    return model, params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ea2291",
      "metadata": {
        "id": "c3ea2291"
      },
      "outputs": [],
      "source": [
        "# vocab size\n",
        "vocab_size= len(char_set)\n",
        "\n",
        "# internal model dimensions\n",
        "d_model=256\n",
        "\n",
        "# number of attention heads\n",
        "n_heads=8\n",
        "\n",
        "# number of Transformer layers\n",
        "n_layers=2\n",
        "\n",
        "# maximum sequence length\n",
        "max_len=128\n",
        "\n",
        "model, params = create_train_state(key, vocab_size, d_model, n_layers, n_heads, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c067e140",
      "metadata": {
        "id": "c067e140"
      },
      "outputs": [],
      "source": [
        "# compute the number of parameters\n",
        "def count_params(params):\n",
        "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
        "print(f\"Number of parameters: {count_params(params):_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52231d73",
      "metadata": {
        "id": "52231d73"
      },
      "outputs": [],
      "source": [
        "# sanity check: create a batch of data & run a forward pass\n",
        "B, T = 4, 32\n",
        "batch = jax.random.randint(\n",
        "    key=key,\n",
        "    shape=(B, T), minval=0, maxval=len(char_set))\n",
        "logits = model.apply({\"params\": params}, batch)\n",
        "\n",
        "print(\"batch shape:\", batch.shape)  # (B, T)\n",
        "print(\"logits shape:\", logits.shape)  # (B, T, vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0213425",
      "metadata": {
        "id": "b0213425"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7a3479",
      "metadata": {
        "id": "ab7a3479"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def loss_and_metrics(logits, targets):\n",
        "    \"\"\"Compute cross-entropy loss and accuracy.\n",
        "\n",
        "    Assumes `targets` contains only valid integer class ids in [0, V-1] (no -1 ignore tokens).\n",
        "\n",
        "    Args:\n",
        "      logits: (B, T, V) float array of unnormalized scores.\n",
        "      targets: (B, T) integer array with ground-truth class ids.\n",
        "\n",
        "    Returns:\n",
        "      loss: scalar average cross-entropy over all positions.\n",
        "      metrics: dict with keys \"loss\" and \"acc\" (both scalars).\n",
        "    \"\"\"\n",
        "    # Flatten batch/time dims so optax works on shape (N, V) and (N,)\n",
        "    vocab = logits.shape[-1]\n",
        "    flat_logits = logits.reshape(-1, vocab)\n",
        "    flat_targets = targets.reshape(-1)\n",
        "\n",
        "    # Per-position cross-entropy, then mean over all positions\n",
        "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
        "    loss = per_pos.mean()\n",
        "\n",
        "    # prediction over all positions\n",
        "    preds = jnp.argmax(logits, axis=-1)  # (B, T)\n",
        "\n",
        "    # compute accuracy over only the last position\n",
        "    is_match = preds == targets\n",
        "\n",
        "    # Accuracy over all positions\n",
        "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
        "\n",
        "    # Accuracy over only last position\n",
        "    acc_last = jnp.mean(is_match.astype(jnp.float32)[:,-1])\n",
        "\n",
        "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05221147",
      "metadata": {
        "id": "05221147"
      },
      "source": [
        "# Optimization step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b720c846",
      "metadata": {
        "id": "b720c846"
      },
      "outputs": [],
      "source": [
        "# create an update function\n",
        "def train_step(params, opt_state, x, y, tx):\n",
        "    \"\"\"Single optimization step using optax optimizer.\n",
        "\n",
        "    Args:\n",
        "      params: pytree of model parameters.\n",
        "      opt_state: optax optimizer state corresponding to `params`.\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "      tx: optax.GradientTransformation (already initialized).\n",
        "\n",
        "    Returns:\n",
        "      new_params: updated parameters after one gradient step.\n",
        "      new_opt_state: updated optimizer state.\n",
        "      metrics: dict of scalar metrics (loss, acc).\n",
        "    \"\"\"\n",
        "    def loss_fn(params):\n",
        "        logits = model.apply({\"params\": params}, x)\n",
        "        loss, metrics = loss_and_metrics(logits, y)\n",
        "        return loss, metrics\n",
        "\n",
        "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
        "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
        "\n",
        "    # optax update: compute parameter updates and new optimizer state\n",
        "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, new_opt_state, metrics\n",
        "\n",
        "# jit: last argument should be static because it is an object\n",
        "train_step = jax.jit(train_step, static_argnames=(\"tx\",))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "269d8e59",
      "metadata": {
        "id": "269d8e59"
      },
      "source": [
        "# Batch creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb30f4e",
      "metadata": {
        "id": "beb30f4e"
      },
      "outputs": [],
      "source": [
        "# create a batch from the training data\n",
        "def get_batch(text_int, B, T):\n",
        "    \"\"\"Create a random batch of data from text_int.\n",
        "\n",
        "    Args:\n",
        "      text_int: 1D array of token ids.\n",
        "      B: batch size (number of sequences).\n",
        "      T: sequence length (number of tokens per sequence).\n",
        "\n",
        "    Returns:\n",
        "      x: (B, T) int array input tokens.\n",
        "      y: (B, T) int array target tokens.\n",
        "    \"\"\"\n",
        "    # choose random starting indices for each sequence in the batch\n",
        "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
        "    # inputs are text from i to i+T\n",
        "    x = np.stack([text_int[i:i+T] for i in ix])\n",
        "    # targets are text from i+1 to i+T+1\n",
        "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
        "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5da80db",
      "metadata": {
        "id": "f5da80db"
      },
      "source": [
        "# Optimizer creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340e8a4c",
      "metadata": {
        "id": "340e8a4c"
      },
      "outputs": [],
      "source": [
        "# define optax optimizer\n",
        "learning_rate = 0.001\n",
        "# Create Adam optimizer (Optax)\n",
        "tx = optax.adam(learning_rate=learning_rate)\n",
        "# Initialize optimizer state for current params\n",
        "opt_state = tx.init(params)\n",
        "print(f\"Initialized optimizer: Adam lr={learning_rate}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f2f1d0",
      "metadata": {
        "id": "28f2f1d0"
      },
      "outputs": [],
      "source": [
        "niter = 100_000\n",
        "B, T = 128, 32\n",
        "loss_history = []\n",
        "time_history = []\n",
        "time_test_history = []\n",
        "loss_test_history = []\n",
        "time_start = time.time()\n",
        "for it in range(niter):\n",
        "    batch = get_batch(train_text_int, B, T)\n",
        "    input, target = batch[0], batch[1]\n",
        "    params_new, opt_state_new, metrics = train_step(params, opt_state, input, target, tx)\n",
        "\n",
        "    # update params and opt_state\n",
        "    params = params_new\n",
        "    opt_state = opt_state_new\n",
        "    acc = metrics['acc']\n",
        "    acc_last = metrics['acc_last']\n",
        "    loss = metrics['loss']\n",
        "\n",
        "    loss_history.append(loss)\n",
        "    time_history.append(time.time() - time_start)\n",
        "\n",
        "    if it % (niter // 50) == 0 or it == niter - 1:\n",
        "        time_since_start = time.time() - time_start\n",
        "        # compute loss on test set\n",
        "        B_test, T_test = 1024, 32\n",
        "        test_batch = get_batch(test_text_int, B_test, T_test)\n",
        "        test_input, test_target = test_batch[0], test_batch[1]\n",
        "        test_logits = model.apply({\"params\": params}, test_input)\n",
        "        test_loss, test_metrics = loss_and_metrics(test_logits, test_target)\n",
        "        test_acc = test_metrics['acc']\n",
        "        test_acc_last = test_metrics['acc_last']\n",
        "        loss_test_history.append(test_loss)\n",
        "        time_test_history.append(time_since_start)\n",
        "        print(f\"iteration {it:_}  time: {time_since_start:.1f} seconds\")\n",
        "        print(f\"\\t \\t loss(train :: test): {loss:.4f} :: {test_loss:.4f}\")\n",
        "        print(f\"\\t \\t accuracy (train :: test): {100*acc:.1f}% :: {100*test_acc:.1f}%\")\n",
        "        print(f\"\\t \\t accuracy (last character) (train :: test): {100*acc_last:.1f}% :: {100*test_acc_last:.1f}%\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "217fb628",
      "metadata": {
        "id": "217fb628"
      },
      "outputs": [],
      "source": [
        "# plot the loss history\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(time_history, loss_history, '-', label='train', color=\"blue\")\n",
        "plt.plot(time_test_history, loss_test_history, '-', label='test', lw=2, color=\"red\")\n",
        "plt.xlabel(\"Time (seconds)\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(\"Training Loss History\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb0b0d1",
      "metadata": {
        "id": "fbb0b0d1"
      },
      "outputs": [],
      "source": [
        "B = 1\n",
        "seed = 42\n",
        "rng = jax.random.PRNGKey(seed)\n",
        "prompt = \"hello my fri\"\n",
        "# prompt_int = encode(prompt.lower())\n",
        "prompt_int = jnp.array([ [char_to_int.get(c, len(char_set)) for c in prompt.lower()[:64]] ], dtype=jnp.int32)\n",
        "\n",
        "gen_len = 1000\n",
        "out_ids = generation.generate_tokens(model, params, rng, prompt_int, gen_len, block_size=64,\n",
        "                          temperature=0.7, sample=True)\n",
        "print('generated ids shape:', out_ids.shape)\n",
        "print('generated text:')\n",
        "generated_text = ''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0]))\n",
        "# concatenate with prompt\n",
        "print(prompt + generated_text)\n",
        "#print(''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0])))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}